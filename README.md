# MGU-V: Lo-Fi Music Generation Using Variational Autoencoders

### Overview
MGU-V is a deep learning-based framework for generating Lo-Fi music using a hybrid approach combining Long Short-Term Memory (LSTM) networks and Variational Autoencoders (VAEs). This model is capable of producing high-quality music by learning from a curated MIDI dataset and generating realistic-sounding Lo-Fi music, which is perfect for use in real-time applications like background scores, video games, or music streaming.

### Features
- **Hybrid Architecture**: Combines the temporal modeling power of LSTMs with the generative capabilities of VAEs.
- **Custom Datasets**: Trained on a merged dataset of over 2300 MIDI files, including datasets like Nottingham Music Database, Maestro Piano Midi, and others.
- **High Performance**: Achieves state-of-the-art performance with an accuracy of 96.2% and minimal loss of 0.19.
- **Lo-Fi Music Generation**: Specifically designed to generate continuous, high-fidelity Lo-Fi music suitable for listening during concentration or background environments.
